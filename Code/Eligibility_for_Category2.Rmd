---
title: "Research About Eligibility for Category 2 Funding"
author: "Bree Norlander"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_notebook
---

```{r}
library(RSocrata)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(aws.s3)
library(geosphere)
library(janitor)
library(stringdist)
library(fuzzyjoin)
```

## Decide between gathering the freshest USAC data:

```{r usac_api_pull}
# # Gather commitments data
# # chosen_category_of_service equal to Category 1,
# # form_471_status_name equal to Committed,
# # form_471_frn_status_name equal to Funded
# cat1_2022 <- read.socrata(
#   "https://opendata.usac.org/resource/avi8-svp9.json?chosen_category_of_service=Category1&funding_year=2022&form_471_status_name=Committed&form_471_frn_status_name=Funded",
#   # Parameters: Category 1, Funding Year, Committed
#   app_token = Sys.getenv("USAC_Socrata")
# )
# 
# # the cat1_2022 list will contain the filtered and calculated data
# cat1_2022 <- cat1_2022 %>%
#   # Add a column indicating how many rows exist for a unique combo of billed entity no and 471 line item no
#   # in other words how many recipients of the commitment will there be
#   # call this column count_ros
#   add_count(billed_entity_number, form_471_line_item_number, name = "count_ros") %>%
#   # Add estimated amount received by individual recipients by dividing
#   # post_discount_extended_eligible_line_item_costs by the count_ros
#   # call this column cat1_discount_by_ros_estimated
#   mutate(
#     cat1_discount_by_ros_estimated = as.numeric(post_discount_extended_eligible_line_item_costs) /
#       count_ros
#   ) %>%
#   # clean text in certain columns - make lowercase and remove trailing spaces
#   mutate(
#     organization_entity_type_name = str_to_lower(str_trim(organization_entity_type_name, side = "both")),
#     ros_entity_type = str_to_lower(str_trim(ros_entity_type, side = "both")),
#     ros_entity_name = str_to_lower(str_trim(ros_entity_name, side = "both")),
#     ros_subtype = str_to_lower(str_trim(ros_subtype, side = "both"))
#   ) %>%
#   # Keep only ros_entity_type libraries or NIFs
#   filter(
#     stringr::str_detect(ros_entity_type, "libr") |
#       ros_entity_type == "non-instructional facility (nif)"
#   ) %>%
#   # Keep ros_entity_type that contain 'libr' OR
#   # NIFs that have libr in the org_entity_type_name OR
#   # NIFs that are part of consortia and have library in the name
#   filter(
#     str_detect(ros_entity_type, "libr") |
#       (
#         ros_entity_type == "non-instructional facility (nif)" &
#           str_detect(organization_entity_type_name, "libr")
#       ) |
#       (
#         ros_entity_type == "non-instructional facility (nif)" &
#           organization_entity_type_name == "consortium" &
#           str_detect(ros_entity_name, "libr")
#       ),
#     # Keep ros_subtypes that are null or NOT public schools
#     (is.na(ros_subtype) |
#        !str_detect(ros_subtype, "public school"))
#   )
# # Write to s3 bucket
# s3write_using(
#   cat1_2022,
#   FUN = write.csv,
#   row.names = F,
#   bucket = "erate-data/data/AVI8-SVP9_Commitments",
#   object = "2022_Libraries_Funded_Committed_Category_1.csv"
# )
# 
# # Read in purpose dataset from USAC
# purpose <- read.socrata(
#   "https://opendata.usac.org/resource/hbj5-2bpj.json?funding_year=2022",
#   app_token = Sys.getenv("USAC_Socrata")
# )
# 
# # Write to s3 bucket
# s3write_using(
#   purpose,
#   FUN = write.csv,
#   row.names = F,
#   bucket = "erate-data/data/HBJ5-2BPJ_Purpose",
#   object = "2022_Full_Purpose.csv"
# )
```

## Or pulling in older USAC data:

```{r}
# Read in 2022 cat1 erate data
cat1_2022 <-
  s3read_using(
    FUN = read.csv,
    na.strings = c("", " ", "N/A", "n/a"),
    object = "2022_Libraries_Funded_Committed_Category_1.csv",
    bucket = "erate-data/data/AVI8-SVP9_Commitments"
  )

# Read in 2022 purpose erate data
purpose <-
  s3read_using(
    FUN = read.csv,
    na.strings = c("", " ", "N/A", "n/a"),
    object = "2022_Full_Purpose.csv",
    bucket = "erate-data/data/HBJ5-2BPJ_Purpose"
  )
```

```{r}
# Read in IMLS PLS AE 2021 dataset stored in S3
imls_ae_2021 <- s3read_using(FUN = read.csv, object = "data/IMLS_PLS/2021_IMLS_PLS_AE.csv", bucket = "erate-data")

# Read in IMLS PLS AE 2021 dataset stored in S3
imls_out_2021 <- s3read_using(FUN = read.csv, object = "data/IMLS_PLS/2021_IMLS_PLS_OUTLET.csv", bucket = "erate-data")
```

```{r}
# How many total outlets in the 2020 PLS
nrow(imls_out_2021)
```

```{r}
# Add in descriptions of locales
imls_out_2021 <- imls_out_2021 |> 
  mutate(
    LOCALE_DESCR = case_when(
      LOCALE == 11 ~ "City Large",
      LOCALE == 12 ~ "City Midsize",
      LOCALE == 13 ~ "City Small",
      LOCALE == 21 ~ "Suburban Large",
      LOCALE == 22 ~ "Suburban Midsize",
      LOCALE == 23 ~ "Suburban Small",
      LOCALE == 31 ~ "Town Fringe",
      LOCALE == 32 ~ "Town Distant",
      LOCALE == 33 ~ "Town Remote",
      LOCALE == 41 ~ "Rural Fringe",
      LOCALE == 42 ~ "Rural Distant",
      LOCALE == 43 ~ "Rural Remote"
    )) %>% 
  # separate to create high-level category
  # https://stackoverflow.com/a/53701998
  separate(LOCALE_DESCR, 
        into = c("LOCALE_TOP_LEVEL_DESCR", NA),
        sep = " ",
        remove = F)
```

```{r}
# How many total outlets in the 2021 PLS that are Branches or Centrals
imls_out_2021 |> 
  #filter to only include central and branch outlets 
  filter(C_OUT_TY %in% c('BR', 'CE')) |> 
  nrow()
```

# Matching PLS Outlets to Erate

```{r}
# Read in matched data stored in S3
# Import hand matching dataset with ros_entity_numbers matched to FSCS keys
hand_matches <-
  s3read_using(
    FUN = read.csv,
    na.strings = c("", " ", "N/A", "n/a"),
    object = "Hand_Matches.csv",
    bucket = "erate-data/data/USAC_IMLS_Match"
  )

hand_matches_no_ae <- hand_matches %>%
  select(ros_entity_number, ros_physical_state, FSCSKEY, FSCS_SEQ) %>%
  mutate(ros_physical_state = toupper(ros_physical_state),
         FSCS_SEQ = as.integer(FSCS_SEQ)) |> 
  filter(!is.na(FSCSKEY) & !is.na(FSCS_SEQ),
         FSCS_SEQ != 999) # we don't want any AE specific matches

# Only want to include hand matches that exist int he 2020 PLS

# Create list
iden_list <- imls_out_2021 |>
  mutate(iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(iden != "NA_NA") |> 
  pull(iden)

# Keep only those that are in the iden_list
hand_matches_no_ae <- hand_matches_no_ae |> 
  mutate(iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(iden %in% iden_list) |> 
  select(-iden)

hand_match_list <- hand_matches_no_ae |> 
  distinct(ros_entity_number) |> 
  pull()

# Create a dataframe of remaining erate libraries to use in matching
erate_libs_for_matching <- cat1_2022 %>%
  filter(
    ros_entity_number != 17012400,
    # this air force base lib doesn't match imls
    ros_entity_number != 137653,
    # this is a regional system that doesn't match imls
    ros_entity_number != 138097,
    # this is a regional system that doesn't match imls
    ros_entity_number != 137724,
    # this is a regional system that doesn't match imls
    ros_entity_number != 231108,
    # this is a regional system that doesn't match imls
    ros_entity_number != 16030444,
    # not an imls library
    ros_entity_number != 126021,
    # this is a regional system that doesn't match imls
    ros_entity_number != 17011387,
    # this is a regional system that doesn't match imls
    ros_entity_number != 16062292,
    # this is a library society that doesn't match imls
    ros_entity_number != 133460,
    # this is a regional system that doesn't match imls
    ros_entity_number != 17009767,
    # this is a federated system that doesn't match imls
    ros_entity_number != 16040215,
    # This is the internet archive and doesn't match imls
    !ros_entity_number %in% hand_match_list
    # we can eliminate all entities in the hand_match_list because we already know the matches
  ) %>%
  # With distinct function, if there are multiple rows for a given combination of inputs,
  # only the first row will be preserved.
  distinct(ros_entity_number, ros_physical_state, .keep_all = T) %>%
  as.data.frame()

# Prepare data for matching
# substring extraction derived from https://rpubs.com/iPhuoc/stringr_manipulation
# stringr and regex help from https://stringr.tidyverse.org/articles/regular-expressions.html
# Eliminate common words in library names like "the" "library" etc.
erate_libs_for_matching <-
  erate_libs_for_matching %>%
  mutate(ros_longitude = as.numeric(ros_longitude),
         ros_latitude = as.numeric(ros_latitude)) %>%
  mutate(
    ros_entity_number = as.numeric(ros_entity_number),
    ros_physical_state = str_to_lower(str_trim(ros_physical_state, side = "both")),
    ros_physical_city = str_to_lower(str_trim(ros_physical_city, side = "both")),
    organization_name = str_to_lower(str_trim(organization_name, side = "both")),
    org_city = str_to_lower(str_trim(org_city, side = "both")),
    org_state = str_to_lower(str_trim(org_state, side = "both")),
    ros_entity_name_processed = str_replace_all(
      ros_entity_name,
      c(
        "the " = "",
        "library" = "",
        "libraries" = "",
        "branch" = "",
        "public" = "",
        "community" = "",
        "\\bbr\\b" = "",
        "\\blib\\b" = ""
      )
    ),
    ros_entity_name_processed = janitor::make_clean_names(ros_entity_name_processed, case =
                                                            "upper_camel"),
    erate_substring = stringr::str_sub(ros_entity_name_processed, 1, 15)
  )

imls_unique_entities <- imls_out_2021 %>%
  mutate(
    LIBNAME = str_to_lower(stringi::stri_enc_toutf8(LIBNAME)),
    STABR = str_to_lower(str_trim(STABR, side = "both")),
    CITY = str_to_lower(stringi::stri_enc_toutf8(CITY)),
    LIBNAME_PROCESSED = str_replace_all(
      LIBNAME,
      c(
        "the " = "",
        "library" = "",
        "libraries" = "",
        "branch" = "",
        "public" = "",
        "community" = "",
        "\\bbr\\b" = "",
        "\\blib\\b" = ""
      )
    ),
    LIBNAME_PROCESSED = janitor::make_clean_names(LIBNAME_PROCESSED, case =
                                                    "upper_camel"),
    SUBSTRING = stringr::str_sub(LIBNAME_PROCESSED, 1, 15)
  )

# Get the list of states that exist in both erate and imls datasets
states_intersect <-
  str_sort(intersect(erate_libs_for_matching[!is.na(erate_libs_for_matching$ros_latitude) &
                                               !is.na(erate_libs_for_matching$ros_longitude), "ros_physical_state"],
                     imls_unique_entities[!is.na(imls_unique_entities$LATITUDE) &
                            !is.na(imls_unique_entities$LONGITUD), "STABR"]))

# create empty list
geo_list = list()

# geo joining on lat/lon between erate data and PLS data
for (i in 1:length(states_intersect)) {
  geo_list[[i]] <- geo_join(
    erate_libs_for_matching %>%
      filter(ros_physical_state == states_intersect[i],
             !(is.na(ros_latitude) | is.na(ros_longitude)
      )),
    imls_unique_entities %>%
      filter(STABR == states_intersect[i],
             !(is.na(LATITUDE) | is.na(LONGITUD))),
    by = c("ros_longitude" = "LONGITUD", "ros_latitude" = "LATITUDE"),
    method = "haversine",
    mode = "inner",
    max_dist = 0.4,
    distance_col = "miles_apart"
  )
}

geo_matches_imls <- bind_rows(geo_list)

# The geo_matches_imls dataset contains many duplicate libraries because multiple libraries from IMLS matched the distance specifications,
# thus duplicating libraries in the USAC data. We need to choose the best of the multiple matches. We'll create a custom algorithm for this
# built by trial and error.
geo_string_match <- geo_matches_imls %>%
  mutate(
    LIBNAME = iconv(LIBNAME, "UTF-8", "UTF-8", sub = ''),
    ADDRESS = iconv(ADDRESS, "UTF-8", "UTF-8", sub = ''),
    ros_physical_address = str_to_lower(str_trim(ros_physical_address, side = "both")),
    ADDRESS = str_to_lower(str_trim(ADDRESS, side = "both")),
    # Add string distance calculations
    sub_dist = stringdist::stringdist(erate_substring, SUBSTRING, method = "jw"),
    name_dist = stringdist::stringdist(ros_entity_name, LIBNAME, method = "jw"),
    add_dist = stringdist::stringdist(ros_physical_address, ADDRESS, method = "jw")
  ) %>%
  rowwise() %>%
  mutate(sum_distances = sum(sub_dist, name_dist, add_dist, na.rm = T)) %>%
  group_by(ros_entity_number) %>%
  arrange(sum_distances) %>%
  slice_min(sum_distances, n = 1) %>%
  slice_min(LIBNAME_PROCESSED, n= 1) |> 
  filter(sum_distances < 1.2 | add_dist < 0.1)

matches <- hand_matches_no_ae |> 
  mutate(ros_physical_state = str_to_lower(str_trim(ros_physical_state, side = "both"))) |> 
  ungroup() |> 
  dplyr::union(geo_string_match |> 
              select(ros_entity_number, ros_physical_state, FSCSKEY, FSCS_SEQ))

# What still doesn't match

rosnomatch <- base::setdiff(unique(erate_libs_for_matching$ros_entity_number),
                            matches$ros_entity_number)

# Make the list into a dataframe
non_matches <-
  data.frame(
    ros_entity_number = matrix(
      unlist(rosnomatch),
      nrow = length(rosnomatch),
      byrow = T
    ),
    stringsAsFactors = FALSE
  )

# Add variables back in to the non-matched recipient numbers
non_matches <- non_matches %>%
  mutate(ros_entity_number = as.numeric(ros_entity_number)) %>%
  # Add in additional information as the dataset is currently just ros_entity_numbers
  left_join(
    cat1_2022 %>%
      mutate(ros_entity_number = as.numeric(ros_entity_number)) %>%
      select(
        ros_entity_number,
        ros_entity_name,
        ros_physical_zipcode,
        ros_entity_type,
        ros_subtype,
        ros_physical_address,
        ros_physical_city,
        ros_physical_state
      ),
    by = "ros_entity_number"
  ) %>%
  distinct(ros_entity_number, .keep_all = T)

# Create a new df to use that starts with the non-matches, adds in processed lib names
# from erate_libs_for_matching, cleans strings
fuzzy_string_test <- non_matches %>%
  mutate(ros_entity_number = as.numeric(ros_entity_number),
         ros_physical_state = str_to_lower(str_trim(ros_physical_state, side = "both")),
         ros_physical_city = str_to_lower(str_trim(ros_physical_city, side = "both"))) %>%
  left_join(
    erate_libs_for_matching %>%
      select(
        ros_entity_number,
        ros_entity_name,
        ros_entity_name_processed,
        erate_substring
      ),
    by = c("ros_entity_number", "ros_entity_name")
    )

# Get the list of zip codes that exist in both datasets
imlszip <-
  intersect(fuzzy_string_test$ros_physical_zipcode,
            imls_unique_entities$ZIP)

# string matching between erate data and IMLS data
name_list = list()

# match on substrings by zipcode
for (i in 1:length(imlszip)) {
  name_list[[i]] <- fuzzy_string_test %>%
    filter(ros_physical_zipcode == imlszip[i]) %>%
    stringdist_join(
      imls_unique_entities %>%
        filter(ZIP == imlszip[i]),
      by = c("ros_entity_name_processed" = "LIBNAME_PROCESSED"),
      max_dist = 0.3,
      mode = "left",
      method = "jw",
      distance_col = "substring_dist"
    )
}

name_matches_zip <- bind_rows(name_list)

# Drop the rows were there was no string match
name_matches_zip <- name_matches_zip |> 
  filter(!is.na(substring_dist))

# Keep the best one of the matches if there are more than one
name_matches_zip <- name_matches_zip %>%
  group_by(ros_entity_number) %>%
  slice_min(substring_dist) %>%
  distinct(ros_entity_number, FSCSKEY, FSCS_SEQ, .keep_all = T)

matches <- matches |> 
  full_join(name_matches_zip |> 
              select(ros_entity_number, ros_physical_state, FSCSKEY, FSCS_SEQ),
            by = c("ros_entity_number", "ros_physical_state", "FSCSKEY", "FSCS_SEQ"))
```

```{r}
# check for REN dupes
matches %>% group_by(ros_entity_number) |> add_tally() |> filter(n>1) |> arrange(ros_entity_number)
```

```{r}
# check for FSCS dupes
matches %>% group_by(FSCSKEY, FSCS_SEQ) |> add_tally() |> filter(n>1) |> arrange(FSCSKEY, FSCS_SEQ)
```

```{r}
# write out FSCS dupes
matches %>% 
  group_by(FSCSKEY, FSCS_SEQ) |> 
  add_tally() |> 
  filter(n>1) |> 
  arrange(FSCSKEY, FSCS_SEQ) |> 
  left_join(cat1_2022 |> distinct(ros_entity_number, ros_entity_name), by="ros_entity_number") |> 
  left_join(imls_out_2021 |> distinct(FSCSKEY, FSCS_SEQ, LIBNAME), by = c("FSCSKEY", "FSCS_SEQ")) |> 
  write.csv("~/Downloads/multiple_FSCS_dupes.csv")
```

```{r}
# Remove the dfs from the matching script
rm(erate_libs_for_matching, fuzzy_string_test, geo_matches_imls, geo_string_match, geo_list, hand_matches, hand_matches_no_ae, imls_unique_entities, name_list, name_matches_zip, non_matches, states_intersect)
```

```{r}
# Now add the matches into the erate dataset and add in the C_OUT_TY field from IMLS so we can filter on it
cat1_2022 <- cat1_2022 |> 
  mutate(ros_entity_number = as.numeric(ros_entity_number)) |> 
  left_join(matches |> select(-ros_physical_state),
            by = "ros_entity_number") |> 
  left_join(imls_out_2021 |> 
              select(FSCSKEY, FSCS_SEQ, C_OUT_TY),
            by = c("FSCSKEY", "FSCS_SEQ")) |>
  relocate(c(FSCSKEY, FSCS_SEQ, C_OUT_TY), .after = ros_entity_number)
```

```{r}
# Chris wants a dataset of counts of imls entities grouped by discount percent
cat1_2022 |> 
  filter(!is.na(FSCSKEY),
         C_OUT_TY %in% c('BR', 'CE')) |> 
  distinct(ros_entity_number, dis_pct) |> 
  group_by(dis_pct) |> 
  summarise(num_libs = n_distinct(ros_entity_number)) |> 
  ungroup() |> 
  mutate(portion_of_total = round(num_libs / sum(num_libs), 2)) |> 
  write.csv("~/Documents/GitHub/E-Rate/Data/2022_Cat1_IMLS-BR-CE_Libs_by_Discount_Percent.csv")
```


```{r}
# Add in POPULSA from IMLS AE
cat1_2022 <- cat1_2022 |> 
  left_join(imls_ae_2021 |> select(FSCSKEY, POPU_LSA), by = "FSCSKEY") |> 
  # and add in Locale's from imls_out
  left_join(imls_out_2021 |> select(FSCSKEY, FSCS_SEQ, LOCALE, LOCALE_DESCR, LOCALE_TOP_LEVEL_DESCR), by = c("FSCSKEY", "FSCS_SEQ"))
```

```{r}
# Add POPULSA from IMLS AE to outlets
imls_out_2021 <- imls_out_2021 |> 
  left_join(imls_ae_2021 |> select(FSCSKEY, POPU_LSA), by = "FSCSKEY") |> 
  # create the population category for imls_outlets
  mutate(pop_category = case_when(
    POPU_LSA == -3 ~ "closedAE",
    POPU_LSA == -9 ~ "suppressed",
    is.na(POPU_LSA) ~ "unknown",
    POPU_LSA < 50000 ~ "under50",
    POPU_LSA >= 50000 ~ "over50"
  ))
```


```{r}
# Normalize the internet speed variable
# Add in FCC benchmark
# Add in BEAD program benchmark variable
cat1_2022 <- cat1_2022 |> 
  mutate(download_speed = as.numeric(download_speed),
         upload_speed = as.numeric(upload_speed)) |>
  mutate(download_speed_mbps = case_when(
    form_471_download_speed_unit_name == "Mbps" ~ download_speed,
    form_471_download_speed_unit_name == "Gbps" ~ download_speed * 1000
  )) %>%
  mutate(upload_speed_mbps = case_when(
    form_471_upload_speed_unit_name == "Mbps" ~ upload_speed,
    form_471_upload_speed_unit_name == "Gbps" ~ upload_speed * 1000
  )) |> 
  mutate(fcc_speed_benchmark = case_when(
    POPU_LSA == -3 ~ "computation error LSA equals -3",
    POPU_LSA == -9 ~ "computation error LSA equals -9",
    is.na(POPU_LSA) ~ "computation error no LSA",
    download_speed_mbps >= 100 & POPU_LSA < 50000 ~ "bandwidth target met",
    download_speed_mbps >= 1000 & POPU_LSA >= 50000 ~ "bandwidth target met",
    download_speed_mbps < 100 ~ "bandwidth target not met",
    download_speed_mbps < 1000 & POPU_LSA >= 50000 ~ "bandwidth target not met"
  )) |> 
  mutate(pop_category = case_when(
    POPU_LSA == -3 ~ "closedAE",
    POPU_LSA == -9 ~ "suppressed",
    is.na(POPU_LSA) ~ "unknown",
    POPU_LSA < 50000 ~ "under50",
    POPU_LSA >= 50000 ~ "over50"
  )) |> 
  mutate(symmetrical_gig = case_when(
    download_speed_mbps >= 1000 & upload_speed_mbps >= 1000 ~ TRUE,
    .default = FALSE
  ))
```

```{r}
# How many outlets in the cat1_2022 dataset right now?
cat1_2022 |> summarise(n_distinct(FSCSKEY, FSCS_SEQ),
                       n_distinct(ros_entity_number))
```


```{r}
# How many outlets in the cat1_2022 dataset right now that aren't IMLS bookmobiles?
cat1_2022 |> filter(is.na(C_OUT_TY) | C_OUT_TY %in% c('BR', 'CE')) |> 
  summarise(n_distinct(FSCSKEY, FSCS_SEQ),
            n_distinct(ros_entity_number))
```

```{r}
# Add a column into imls_out_2021 indicating if the outlet rec'd a Cat1 commitment
imls_out_2021 <- imls_out_2021 |> 
  left_join(cat1_2022 |>
              filter(ros_entity_type != "non-instructional facility (nif)") |> 
              distinct(FSCSKEY, FSCS_SEQ, Cat1_Commitment_2022 = TRUE),
            by = c("FSCSKEY", "FSCS_SEQ")) |> 
  mutate(Cat1_Commitment_2022 = ifelse(is.na(Cat1_Commitment_2022), FALSE, Cat1_Commitment_2022))
```

```{r}
# Save the imls_out_2021 file to Github for reference
write.csv(imls_out_2021, "~/Documents/GitHub/E-Rate/Data/2021_IMLS_CEandBR_Outlets_with_2022_Erate_Cat1_Commitment_Binary_Column.csv")
```

```{r}
# How many outlets that are branches or centrals rec'd CAt1 commitments?
imls_out_2021 |> 
  filter(Cat1_Commitment_2022 == TRUE,
         C_OUT_TY %in% c('BR', 'CE')) |> 
  nrow()
```

```{r}
# Join cat1_2022 to purpose dataset
cat1_2022 <- cat1_2022 |> 
  mutate_at(c('application_number', 'form_471_line_item_number'), as.numeric) |> 
  left_join((purpose %>% 
               select(application_number, form_471_line_item_number, form_471_purpose_name, form_version, connection_directly_school, connection_supports_service) %>% 
               filter(form_version == "Current") %>% 
               mutate(application_number = as.numeric(application_number),
                      form_471_line_item_number = as.numeric(form_471_line_item_number))
             ), 
            by=c("application_number", "form_471_line_item_number")) 
```

```{r}
# Count the distinct imls entities in the dataset we will create below
cat1_2022 %>% 
  mutate(imls_iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(!is.na(FSCSKEY),
         (grepl('^Internet access', form_471_purpose_name) | grepl('^Data connection(s)', form_471_purpose_name)),
         form_471_function_name != "Miscellaneous",
         form_471_product_name != "Data plan for portable device",
         !is.na(form_471_download_speed_unit_name),
         ros_entity_type != "non-instructional facility (nif)",
         C_OUT_TY %in% c('BR', 'CE')
         ) |> 
  summarise(n_distinct(imls_iden, na.rm = T), n_distinct(ros_entity_number, na.rm = TRUE))
```

```{r}
# Create a dataset that only includes the two purposes that begin with the words Internet access 
# and the one that begins with Data connection(s)
# Use only PLS libs that are branches or centrals

# One dataset for those entites with a single application
ia_purposes_singleapp <- cat1_2022 %>% 
  mutate(imls_iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(!is.na(FSCSKEY),
         (grepl('^Internet access', form_471_purpose_name) | grepl('^Data connection(s)', form_471_purpose_name)),
         form_471_function_name != "Miscellaneous",
         form_471_product_name != "Data plan for portable device",
         !is.na(form_471_download_speed_unit_name),
         ros_entity_type != "non-instructional facility (nif)",
         C_OUT_TY %in% c('BR', 'CE')
         ) |> 
  add_count(imls_iden, name = "how_many_rows_per_lib") |> 
  filter(how_many_rows_per_lib == 1)
```

```{r}
# One dataset with entities that have multiple apps - we will filter this down to one app
ia_purposes_multiapp <- cat1_2022 %>% 
  mutate(imls_iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(!is.na(FSCSKEY),
         (grepl('^Internet access', form_471_purpose_name) | grepl('^Data connection(s)', form_471_purpose_name)),
         form_471_function_name != "Miscellaneous",
         form_471_product_name != "Data plan for portable device",
         !is.na(form_471_download_speed_unit_name),
         ros_entity_type != "non-instructional facility (nif)",
         C_OUT_TY %in% c('BR', 'CE')
         ) |> 
  add_count(imls_iden, name = "how_many_rows_per_lib") |> 
  filter(how_many_rows_per_lib > 1)

done1 <- ia_purposes_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |> 
  filter(how_many_rows_per_lib_2 == 1)

done2 <- ia_purposes_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 == 1)

done3 <- ia_purposes_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 > 1) |> 
  mutate(function_rank = case_when(
    form_471_function_name == "Fiber" ~ 1,
    form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ 2,
    .default = 9
  )) |> 
  group_by(imls_iden) |> 
  slice_min(function_rank, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_4") |> 
  filter(how_many_rows_per_lib_4 == 1)

done4 <- ia_purposes_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 > 1) |> 
  mutate(function_rank = case_when(
    form_471_function_name == "Fiber" ~ 1,
    form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ 2,
    .default = 9
  )) |> 
  group_by(imls_iden) |> 
  slice_min(function_rank, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_4") |> 
  filter(how_many_rows_per_lib_4 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(total_monthly_cost, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_5") |> 
  filter(how_many_rows_per_lib_5 == 1)

done5 <- ia_purposes_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 > 1) |> 
  mutate(function_rank = case_when(
    form_471_function_name == "Fiber" ~ 1,
    form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ 2,
    .default = 9
  )) |> 
  group_by(imls_iden) |> 
  slice_min(function_rank, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_4") |> 
  filter(how_many_rows_per_lib_4 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(total_monthly_cost, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_5") |> 
  filter(how_many_rows_per_lib_5 > 1) |> 
  group_by(imls_iden) |> 
  slice_head(n=1) 
```

```{r}
# bind all the single entity dataframes
ia_purposes <- rbind(ia_purposes_singleapp,
      done1 |> 
        select(-how_many_rows_per_lib_2),
      done2 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3),
      done3 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3, -how_many_rows_per_lib_4, -function_rank),
      done4 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3, -how_many_rows_per_lib_4, -function_rank, -how_many_rows_per_lib_5),
      done5 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3, -how_many_rows_per_lib_4, -function_rank, -how_many_rows_per_lib_5)
)
```

```{r}
# Count distinct - we want same number of imls_iden as before the filtering
ia_purposes |> 
  summarise(n_distinct(imls_iden, na.rm = T), n_distinct(ros_entity_number, na.rm = TRUE))
```

# Chris J's BEAD Classification:

Class 1 - these are libraries that currently have a 1Gbps symmetrical connection regardless of the technology utilized. If they are reporting 1Gbps in download/upload speeds fields then they fall into this class.

Class 2 - these are libraries that do not currently have a 1Gbps symmetrical connection, but have a current connection type that would allow for an upgrade to a 1Gbps symmetrical connection. These libraries report less than 1Gbps in download OR upload speeds and have the following types of function + product combinations:
	
Function Name (form_471_function_name) | Product Name (form_471_product_name)
Fiber | *(Any)
Copper | Cable Modem
Copper | Switched Multimegabit Data System
Other | Other
Other  | Broadband Over Power Lines
Other | Radio Loop
Wireless | Microwave

Class 3 - these are libraries that do not currently have a 1Gbps symmetrical connection, AND do not have a current connection type that would allow for an upgrade to a 1Gbps symmetrical connection. These libraries report less than 1Gbps in download OR upload speeds and have any of the types of function + product combinations not included in Class 2.

```{r}
# Create a BEAD Class based on CJs rules
ia_purposes <- ia_purposes |> 
  mutate(bead_class = case_when(
    symmetrical_gig == TRUE ~ "class1",
    symmetrical_gig == FALSE & form_471_function_name == "Fiber" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Copper" & form_471_product_name == "Switched Multimegabit Data System" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Other" & form_471_product_name == "Other" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Other" & form_471_product_name == "Broadband Over Power Lines" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Other" & form_471_product_name == "Radio Loop" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Wireless" & form_471_product_name == "Microwave" ~ "class2",
    .default = "class3"
  ))
```


```{r}
# Save the ia_purposes dataset file to Github for reference
write.csv(ia_purposes, "~/Documents/GitHub/E-Rate/Data/2022_Internet_Access_with_Purpose_2021_IMLS_Entities_Single_Row.csv")
```

```{r}
# Check to see how the purpose filtering affects a few other columns
print(unique(ia_purposes$form_471_function_name))
print(unique(ia_purposes$form_471_product_name))
print(unique(ia_purposes$form_471_download_speed_unit_name))
```

```{r}
# How many libs meet the fcc benchmark based on population
ia_purposes |> 
  select(ros_entity_number, imls_iden, pop_category, fcc_speed_benchmark) |> 
  group_by(pop_category, fcc_speed_benchmark) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# How many libs meet the fcc benchmark based on locale
ia_purposes |> 
  select(ros_entity_number, imls_iden, LOCALE_TOP_LEVEL_DESCR, fcc_speed_benchmark) |> 
  group_by(LOCALE_TOP_LEVEL_DESCR, fcc_speed_benchmark) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# How many libs meet the fcc benchmark based on function
ia_purposes |> 
  select(ros_entity_number, imls_iden, form_471_function_name, fcc_speed_benchmark) |> 
  group_by(form_471_function_name, fcc_speed_benchmark) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# How many libs are class 3 for BEAD
ia_purposes |> 
  select(imls_iden, bead_class) |> 
  group_by(bead_class) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# Save the class3 libraries as a file to Github for reference
ia_purposes |> 
  filter(bead_class == "class3") |> 
  select(ros_entity_number, FSCSKEY, FSCS_SEQ, ros_entity_name, ros_physical_city, ros_physical_state, application_number, form_471_line_item_number, upload_speed_mbps, download_speed_mbps, form_471_function_name, form_471_product_name) |>
  write.csv("~/Documents/GitHub/E-Rate/Data/2022_Libraries_without_1G_symmetrical_capability.csv")
```

```{r}
# Write out a csv for Chris
# Include lat/lon, FCC, BEAD, connection types
ia_purposes |> 
  select(ros_entity_number, 
         FSCSKEY, 
         FSCS_SEQ, 
         ros_entity_name, 
         ros_physical_address, 
         ros_physical_city, 
         ros_physical_state, 
         ros_physical_zipcode,
         ros_latitude,
         ros_longitude,
         organization_entity_type_name,
         application_number,
         funding_request_number,
         form_471_line_item_number,
         spin_name,
         form_471_service_type_name,
         form_471_function_name,
         form_471_product_name,
         form_471_frn_fiber_type_name,
         form_471_frn_fiber_sub_type_name,
         form_471_purpose_name,
         download_speed_mbps,
         upload_speed_mbps,
         POPU_LSA,
         LOCALE,
         fcc_speed_benchmark,
         pop_category,
         symmetrical_gig,
         bead_class) |> 
  write.csv("~/Documents/GitHub/E-Rate/Data/2022_Internet_Access_with_Purpose_2021_IMLS_Entities_Single_Row_Minified.csv")
```


# Fourth purpose
## Data Connection between two or more sites entirely within the applicant’s network

```{r}
# Create a dataset that only includes the purpose called 
# "Data Connection between two or more sites entirely within the applicant’s network"
# Use only PLS libs

# One dataset for those entites with a single application
ia_4purpose_singleapp <- cat1_2022 %>% 
  mutate(imls_iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(!is.na(FSCSKEY),
         grepl('^Data Connection between two or more', form_471_purpose_name),
         form_471_function_name != "Miscellaneous",
         form_471_product_name != "Data plan for portable device",
         !is.na(form_471_download_speed_unit_name),
         ros_entity_type != "non-instructional facility (nif)",
         C_OUT_TY %in% c('BR', 'CE')
         ) |> 
  add_count(imls_iden, name = "how_many_rows_per_lib") |> 
  filter(how_many_rows_per_lib == 1)
```

```{r}
# One dataset with entities that have multiple apps - we will filter this down to one app
ia_4purpose_multiapp <- cat1_2022 %>% 
  mutate(imls_iden = paste(FSCSKEY, FSCS_SEQ, sep = "_")) |> 
  filter(!is.na(FSCSKEY),
         grepl('^Data Connection between two or more', form_471_purpose_name),
         form_471_function_name != "Miscellaneous",
         form_471_product_name != "Data plan for portable device",
         !is.na(form_471_download_speed_unit_name),
         ros_entity_type != "non-instructional facility (nif)",
         C_OUT_TY %in% c('BR', 'CE')
         ) |> 
  add_count(imls_iden, name = "how_many_rows_per_lib") |> 
  filter(how_many_rows_per_lib > 1)

complete1 <- ia_4purpose_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |> 
  filter(how_many_rows_per_lib_2 == 1)

complete2 <- ia_4purpose_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 == 1)

complete3 <- ia_4purpose_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 > 1) |> 
  mutate(function_rank = case_when(
    form_471_function_name == "Fiber" ~ 1,
    form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ 2,
    .default = 9
  )) |> 
  group_by(imls_iden) |> 
  slice_min(function_rank, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_4") |> 
  filter(how_many_rows_per_lib_4 == 1)

complete4 <- ia_4purpose_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 > 1) |> 
  mutate(function_rank = case_when(
    form_471_function_name == "Fiber" ~ 1,
    form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ 2,
    .default = 9
  )) |> 
  group_by(imls_iden) |> 
  slice_min(function_rank, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_4") |> 
  filter(how_many_rows_per_lib_4 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(total_monthly_cost, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_5") |> 
  filter(how_many_rows_per_lib_5 == 1)

complete5 <- ia_4purpose_multiapp |> 
  group_by(imls_iden) |> 
  slice_max(download_speed_mbps, with_ties = T) |> 
  ungroup() |> 
  add_count(imls_iden, name = "how_many_rows_per_lib_2") |>  
  filter(how_many_rows_per_lib_2 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(upload_speed_mbps, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_3") |> 
  filter(how_many_rows_per_lib_3 > 1) |> 
  mutate(function_rank = case_when(
    form_471_function_name == "Fiber" ~ 1,
    form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ 2,
    .default = 9
  )) |> 
  group_by(imls_iden) |> 
  slice_min(function_rank, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_4") |> 
  filter(how_many_rows_per_lib_4 > 1) |> 
  group_by(imls_iden) |> 
  slice_max(total_monthly_cost, with_ties = T) |> 
  ungroup() |>
  add_count(imls_iden, name = "how_many_rows_per_lib_5") |> 
  filter(how_many_rows_per_lib_5 > 1) |> 
  group_by(imls_iden) |> 
  slice_head(n=1) 
```

```{r}
# bind all the single entity dataframes
ia_4purpose <- rbind(ia_4purpose_singleapp,
      complete1 |> 
        select(-how_many_rows_per_lib_2),
      complete2 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3),
      complete3 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3, -how_many_rows_per_lib_4, -function_rank),
      complete4 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3, -how_many_rows_per_lib_4, -function_rank, -how_many_rows_per_lib_5),
      complete5 |> 
        select(-how_many_rows_per_lib_2, -how_many_rows_per_lib_3, -how_many_rows_per_lib_4, -function_rank, -how_many_rows_per_lib_5)
)
```

```{r}
# Create a list of RENs in the ia_purposes dataset (for filtering)
ia_purposes_list <- ia_purposes |> 
  select(ros_entity_number) |> 
  pull()
```

```{r}
# We don't want any entities from the ia_purposes_list in the ia_4purpose dataset
ia_4purpose <- ia_4purpose |> 
  filter(!ros_entity_number %in% ia_purposes_list)
```

```{r}
# Count distinct - we want same number of imls_iden as before the filtering
ia_4purpose |> 
  summarise(n_distinct(imls_iden, na.rm = T), n_distinct(ros_entity_number, na.rm = TRUE))
```

```{r}
rm(complete1, complete2, complete3, complete4, complete5, done1, done2, done3, done4, done5)
```

# Chris J's BEAD Classification:

Class 1 - these are libraries that currently have a 1Gbps symmetrical connection regardless of the technology utilized. If they are reporting 1Gbps in download/upload speeds fields then they fall into this class.

Class 2 - these are libraries that do not currently have a 1Gbps symmetrical connection, but have a current connection type that would allow for an upgrade to a 1Gbps symmetrical connection. These libraries report less than 1Gbps in download OR upload speeds and have the following types of function + product combinations:
	
Function Name (form_471_function_name) | Product Name (form_471_product_name)
Fiber | *(Any)
Copper | Cable Modem
Copper | Switched Multimegabit Data System
Other | Other
Other  | Broadband Over Power Lines
Other | Radio Loop
Wireless | Microwave

Class 3 - these are libraries that do not currently have a 1Gbps symmetrical connection, AND do not have a current connection type that would allow for an upgrade to a 1Gbps symmetrical connection. These libraries report less than 1Gbps in download OR upload speeds and have any of the types of function + product combinations not included in Class 2.

```{r}
# Create a BEAD Class based on CJs rules
ia_4purpose <- ia_4purpose |> 
  mutate(bead_class = case_when(
    symmetrical_gig == TRUE ~ "class1",
    symmetrical_gig == FALSE & form_471_function_name == "Fiber" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Copper" & form_471_product_name == "Cable Modem" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Copper" & form_471_product_name == "Switched Multimegabit Data System" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Other" & form_471_product_name == "Other" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Other" & form_471_product_name == "Broadband Over Power Lines" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Other" & form_471_product_name == "Radio Loop" ~ "class2",
    symmetrical_gig == FALSE & form_471_function_name == "Wireless" & form_471_product_name == "Microwave" ~ "class2",
    .default = "class3"
  ))
```

```{r}
# Check to see how the purpose filtering affects a few other columns
print(unique(ia_4purpose$form_471_function_name))
print(unique(ia_4purpose$form_471_product_name))
print(unique(ia_4purpose$form_471_download_speed_unit_name))
```

```{r}
# How many libs meet the fcc benchmark based on population
ia_4purpose |> 
  select(ros_entity_number, imls_iden, pop_category, fcc_speed_benchmark) |> 
  group_by(pop_category, fcc_speed_benchmark) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# How many libs meet the fcc benchmark based on locale
ia_4purpose |> 
  select(ros_entity_number, imls_iden, LOCALE_TOP_LEVEL_DESCR, fcc_speed_benchmark) |> 
  group_by(LOCALE_TOP_LEVEL_DESCR, fcc_speed_benchmark) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# How many libs meet the fcc benchmark based on function
ia_4purpose |> 
  select(ros_entity_number, imls_iden, form_471_function_name, fcc_speed_benchmark) |> 
  group_by(form_471_function_name, fcc_speed_benchmark) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# How many libs are class 3 for BEAD
ia_4purpose |> 
  select(imls_iden, bead_class) |> 
  group_by(bead_class) |> 
  summarise(n_distinct(imls_iden))
```

```{r}
# Count how many entities have 4th purpose only by state
ia_4purpose |> 
  #group_by(ros_physical_state) |> 
  count(ros_physical_state)
```

